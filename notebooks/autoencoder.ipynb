{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using `xbatcher` to train an autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from importlib import reload\n",
    "\n",
    "# DL stuff\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# Geospatial stuff\n",
    "import xarray as xr\n",
    "import xbatcher\n",
    "import rioxarray\n",
    "import xbatcher\n",
    "from xbatcher.loaders.torch import MapDataset\n",
    "\n",
    "# Etc\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# Locals\n",
    "import functions\n",
    "import autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by pulling a segment of NASADEM for Washington's Olympic peninsula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasterio adds a blank edge. Trim these out.\n",
    "dem = rioxarray.open_rasterio(\"../ASTGTMV003_N47W124_dem.tif\")\n",
    "dem = dem.isel(y=slice(0, -1), x=slice(0, -1))\n",
    "dem = (dem - dem.min()) / (dem.max() - dem.min())\n",
    "dem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem.isel(band=0).plot.imshow(cmap=\"terrain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate training examples\n",
    "Here, we use xbatcher to window patches of terrain in the same location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgen_x = xbatcher.BatchGenerator(\n",
    "    dem,\n",
    "    input_dims=dict(x=32, y=32),\n",
    "    input_overlap=dict(x=16, y=16)\n",
    ")\n",
    "\n",
    "ds = MapDataset(\n",
    "    X_generator=bgen_x,)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(ds, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = next(iter(loader))\n",
    "\n",
    "print(\"Input tensor shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = autoencoder.Autoencoder(base_channel_size=32, latent_dim=64, num_input_channels=1, width=32, height=32)\n",
    "opt = m._configure_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = m(X)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "We aren't using pytorch-lightning and load a pre-trained model here to keep the notebook environment lean. For your project, we highly recommend using a framework to abstract away much of the boilerplate code below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def train_one_epoch(epoch_index):\n",
    "    last_loss = 0.\n",
    "    running_loss = 0.\n",
    "    \n",
    "    for i, batch in enumerate(tqdm(loader)):\n",
    "        # Zero your gradients for every batch!\n",
    "        opt.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = m(batch)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = m._get_reconstruction_loss(batch, outputs)\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        opt.step()\n",
    "\n",
    "    return running_loss / len(loader)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_epochs = 5\n",
    "for i_epoch in range(n_epochs):\n",
    "    loss = train_one_epoch(i_epoch)\n",
    "    print(f\"Epoch {i_epoch+1:>3}: {loss:.3e}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "torch.save(m.state_dict(), \"../autoencoder.torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{danger}\n",
    "This model is certainly overfitted. For brevity we have omitted a validation dataset, which is essential for building models that generalize well on unseen data.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.load_state_dict(torch.load(\"../autoencoder.torch\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.eval()\n",
    "n_examples = 4\n",
    "inputs = next(iter(loader))\n",
    "outputs = m(inputs)\n",
    "\n",
    "inputs = inputs.detach().cpu().numpy()\n",
    "outputs = outputs.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(n_examples, 2)\n",
    "\n",
    "for i_row in range(n_examples):\n",
    "    axes[i_row, 0].imshow(inputs[i_row, 0, ...])\n",
    "    axes[i_row, 1].imshow(outputs[i_row, 0, ...])\n",
    "\n",
    "for a in axes.flat:\n",
    "    a.set_xticks([])\n",
    "    a.set_yticks([])\n",
    "\n",
    "axes[0, 0].set_title(\"Original patch\")\n",
    "axes[0, 1].set_title(\"Reconstruction\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction 1: Getting the full array back\n",
    "\n",
    "Suppose we would like to evaluate how the autoencoder does on reconstructing the entire terrain patch by combining outputs across all input patches. To do so we can use the `predict_on_array` function described in the previous notebook. Our model outputs tensors with shape `(band=1, x=32, y=32)`. We need to specify each of these axes in the call to `predict_on_array`. `channel` does not change size and is not used by the `BatchGenerator`, so it goes in `core_dim`. Both `x` and `y` are used by the `BatchGenerator`, so although they do not change size they still go in `resample_dim`. That accounts for all tensor axes, so we can leave the `new_dim` argument as an empty list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_reconst = functions.predict_on_array(\n",
    "    dataset=ds,\n",
    "    model=m,\n",
    "    output_tensor_dim=dict(band=1, y=32, x=32),\n",
    "    new_dim=[],\n",
    "    core_dim=[\"band\"],\n",
    "    resample_dim=[\"x\", \"y\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_reconst.isel(band=0).plot.imshow(cmap=\"terrain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That certainly looks like the original DEM. Let's try plotting the error in the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = (dem_reconst - dem)\n",
    "err.isel(band=0).plot.imshow()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err.plot.hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction 2: Getting the latent dimension\n",
    "\n",
    "A common application of autoencoders is to use the latent dimension for some application. Let's turn our autoencoder's predictions into a data cube. To do so we will modify the batch generator to not have overlapping windows. We also have to slightly clip the size of the input DEM. This is because we are effectively downscaling the spatial axes by a factor of 32. Since `3600 / 32` is not an integer, `predict_on_array` will not know how to rescale the array size. So, we have to clip the DEM to the nearest integer multiple of 32. In this case the nearest multiple is 3584, which we achieve by clipping 8 pixels from each side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgen_no_overlap = xbatcher.BatchGenerator(\n",
    "    dem.isel(x=slice(8, -8), y=slice(8, -8)),\n",
    "    input_dims=dict(x=32, y=32),\n",
    "    input_overlap=dict(x=0, y=0)\n",
    ")\n",
    "\n",
    "ds_no_overlap = MapDataset(\n",
    "    X_generator=bgen_no_overlap\n",
    ")\n",
    "\n",
    "loader = torch.utils.data.DataLoader(ds_no_overlap, batch_size=16, shuffle=True)\n",
    "\n",
    "ex_input = next(iter(loader))\n",
    "\n",
    "# Same as before\n",
    "print(\"Input shape:\", ex_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will write a function that the calls the encoder arm of the autoencoder and adds a fake x and y dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_with_encoder(x):\n",
    "    return m.encoder(x)[:, None, None, :]\n",
    "\n",
    "ex_output = infer_with_encoder(ex_input)\n",
    "print(\"Output shape:\", ex_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we combine the outputs together into a new data cube."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim_cube = functions.predict_on_array(\n",
    "    dataset=ds_no_overlap,\n",
    "    model=infer_with_encoder,\n",
    "    output_tensor_dim=dict(y=1, x=1, channel=64),\n",
    "    new_dim=[\"channel\"],\n",
    "    core_dim=[],\n",
    "    resample_dim=[\"x\", \"y\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that despite substantially re-arranging the input `DataArray`, we have retained the coordinate information at a resampled resolution.\n",
    "\n",
    "If we simply sum the output over the channel dimension, we see that the encoder clearly distinguishes between upland and lowland areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim_cube.sum(dim=\"channel\").plot.imshow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final demonstration of this workflow, let's compute the cosine similarity of each of the below pixels with the latent encoding of [Mt. Olympus](https://en.wikipedia.org/wiki/Mount_Olympus_(Washington))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olympus = dict(x=-123.7066, y=47.7998)\n",
    "olympus_latent = latent_dim_cube.sel(**olympus, method=\"nearest\")\n",
    "olympus_latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_cosine_similarity(x, y):\n",
    "    return np.dot(x, y)/(norm(x)*norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olympus_similarity = xr.apply_ufunc(\n",
    "    numpy_cosine_similarity,\n",
    "    latent_dim_cube,\n",
    "    input_core_dims = [[\"channel\"]],\n",
    "    output_core_dims = [[]],\n",
    "    vectorize=True,\n",
    "    kwargs=dict(y=olympus_latent.data)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olympus_similarity.plot.imshow()\n",
    "plt.scatter(olympus[\"x\"], olympus[\"y\"], marker=\"*\", c=\"purple\", edgecolor=\"black\", s=200)\n",
    "plt.title(\"Cosine similarity with Mt. Olympus, WA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can identify foothills with similar topography to Grisdale, WA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grisdale = dict(y=47.356625678465925, x=-123.61183314426664)\n",
    "grisdale_latent = latent_dim_cube.sel(**grisdale, method=\"nearest\")\n",
    "\n",
    "grisdale_similarity = xr.apply_ufunc(\n",
    "    numpy_cosine_similarity,\n",
    "    latent_dim_cube,\n",
    "    input_core_dims = [[\"channel\"]],\n",
    "    output_core_dims = [[]],\n",
    "    vectorize=True,\n",
    "    kwargs=dict(y=grisdale_latent.data)\n",
    ")\n",
    "\n",
    "grisdale_similarity.plot.imshow()\n",
    "plt.scatter(grisdale[\"x\"], grisdale[\"y\"], marker=\"*\", c=\"purple\", edgecolor=\"black\", s=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This result is admittedly very similar to if we had just selected elevation bands :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Our goal with this notebook has been to show how xbatcher supports linking `xarray` objects with deep learning models, and with converting model output back into labeled `xarray` objects. We have demonstrated two examples of reconstructing model output, both when tensor shape changes and when it does not.\n",
    "\n",
    "If you encounter any issues, please open an issue on the GitHub repository for this cookbook. Other feedback is welcome!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python 3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "op": "addrange",
         "valuelist": [
          "Python3"
         ]
        },
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "display_name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ]
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
